# Initialization of Parameters in Deep Neural Networks

## Aim
To understand the importance of parameter initialization in deep neural networks and analyze its effect on training performance and convergence.

## Description
This experiment implements a deep neural network from scratch using Python. The network consists of multiple hidden layers with ReLU activation and a Sigmoid output layer. Forward propagation, backward propagation, loss computation, and parameter updates are manually implemented to study how proper initialization impacts learning.

## Files Included
- Initialization.ipynb  
  Main Jupyter Notebook containing the complete experiment, execution, and results.

- init.py  
  Contains core neural network functions such as forward propagation, backward propagation, loss computation, and parameter updates.

- init_utils.py  
  Utility functions for activation functions, dataset loading, prediction, and plotting decision boundaries.

## Tools and Libraries Used
- Python
- NumPy
- Matplotlib
- scikit-learn
- h5py

## How to Run
1. Open `Initialization.ipynb` using Jupyter Notebook.
2. Run all cells sequentially.
3. Observe the outputs, accuracy, and plots generated.

## Outcome
The experiment successfully demonstrates that proper initialization of parameters helps improve convergence speed and overall performance of deep neural networks.
